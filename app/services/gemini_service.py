# Gemini AI Service Integration
import json
import asyncio
import os
from typing import Dict, List, Optional, Any, Union
from datetime import datetime

# Google AI imports (using existing stable API)
try:
    import google.generativeai as genai
    from google.generativeai.types import HarmCategory, HarmBlockThreshold
    GOOGLE_AI_AVAILABLE = True
except ImportError:
    GOOGLE_AI_AVAILABLE = False
    print("Warning: Google AI not available - Gemini features disabled")

import io
try:
    from PIL import Image as PILImage
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

from sqlalchemy.ext.asyncio import AsyncSession
from dotenv import load_dotenv

from app.core.config import settings
from app.db.crud_enhanced import log_system_event, save_chat_to_history

# Load environment variables
load_dotenv(".env")

class GeminiService:
    """Google Gemini AI Integration Service with new API client"""
    
    def __init__(self):
        """Initialize Gemini service with API configuration"""
        self.api_key = os.environ.get("GEMINI_API_KEY") or getattr(settings, 'GEMINI_API_KEY', None)
        self.model_name = getattr(settings, 'GEMINI_MODEL', 'gemini-1.5-flash')  # Use flash model for better availability
        self.temperature = getattr(settings, 'GEMINI_TEMPERATURE', 0.7)
        self.max_tokens = getattr(settings, 'GEMINI_MAX_TOKENS', 1000)
        self.enable_safety = getattr(settings, 'GEMINI_ENABLE_SAFETY', False)  # Disable safety for testing
        
        # Check if Google AI is available
        if not GOOGLE_AI_AVAILABLE:
            self.client = None
            self.chat = None
            print("âš ï¸ Google Genai library not available - Gemini features disabled")
            return
            
        # Configure Gemini API
        if self.api_key:
            self._initialize_service()
        else:
            self.model = None
            self.chat_sessions = {}
            print("Warning: No Gemini API key found - Gemini features disabled")
            
        # Chat sessions for different users (manual conversation tracking)
        self.chat_sessions: Dict[str, Any] = {}
        
    def _initialize_service(self):
        """Initialize the Gemini service with existing stable API"""
        if not GOOGLE_AI_AVAILABLE:
            self.model = None
            return
            
        try:
            # Configure the API
            genai.configure(api_key=self.api_key)
            
            # Safety settings - disabled for testing HR/Government chatbot
            safety_settings = {
                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
            } if self.enable_safety else None
            
            # Generation configuration
            generation_config = genai.types.GenerationConfig(
                temperature=self.temperature,
                max_output_tokens=self.max_tokens,
                top_p=0.8,
                top_k=40
            )
            
            # Initialize model
            self.model = genai.GenerativeModel(
                model_name=self.model_name,
                generation_config=generation_config,
                safety_settings=safety_settings
            )
            
            print(f"Success: Gemini service initialized with model {self.model_name}")
            
        except Exception as e:
            print(f"Failed to initialize Gemini service: {e}")
            self.model = None
    
    def is_available(self) -> bool:
        """Check if Gemini service is available"""
        return GOOGLE_AI_AVAILABLE and self.model is not None and bool(self.api_key)
    
    async def generate_response(
        self, 
        user_message: str, 
        user_id: str, 
        use_session: bool = True
    ) -> Dict[str, Any]:
        """
        Generate AI response using Gemini with chat session
        
        Args:
            user_message: User's input message
            user_id: LINE user ID for session tracking
            use_session: Whether to use chat session for continuity
            
        Returns:
            Dict containing response, metadata, and status
        """
        if not self.is_available():
            return {
                "success": False,
                "response": "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¸£à¸°à¸šà¸š AI à¹„à¸¡à¹ˆà¸žà¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¹ƒà¸™à¸‚à¸“à¸°à¸™à¸µà¹‰",
                "error": "Gemini service not available",
                "usage": None
            }
        
        try:
            # Generate response with conversation context
            if use_session:
                response = await self._generate_with_context_async(user_id, user_message)
            else:
                # Simple generation without context
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(
                    None, 
                    lambda: self.model.generate_content(user_message)
                )
                response = self._extract_response_text(result)
            
            if response:
                return {
                    "success": True,
                    "response": response,
                    "model": self.model_name,
                    "usage": {
                        "prompt_tokens": len(user_message.split()),
                        "completion_tokens": len(response.split()),
                        "total_tokens": len(user_message.split()) + len(response.split())
                    },
                    "timestamp": datetime.now().isoformat()
                }
            else:
                return {
                    "success": False,
                    "response": "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸³à¸•à¸­à¸šà¹„à¸”à¹‰à¹ƒà¸™à¸‚à¸“à¸°à¸™à¸µà¹‰",
                    "error": "Empty response from Gemini",
                    "usage": None
                }
                
        except Exception as e:
            error_msg = str(e)
            print(f"Gemini generation error: {error_msg}")
            return {
                "success": False,
                "response": "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥",
                "error": error_msg,
                "usage": None
            }
    
    def _extract_response_text(self, response) -> Optional[str]:
        """Extract text from Gemini response with better error handling"""
        if not response:
            return None
            
        try:
            # Try the quick accessor first
            if hasattr(response, 'text') and response.text:
                return response.text.strip()
        except Exception as e:
            print(f"Quick accessor failed: {e}")
            
        try:
            # Check finish reason for safety filtering
            if hasattr(response, 'candidates') and response.candidates:
                candidate = response.candidates[0]
                if hasattr(candidate, 'finish_reason'):
                    print(f"Finish reason: {candidate.finish_reason}")
                    
                    # Try to extract content even if finish_reason indicates issues
                    if hasattr(candidate, 'content') and candidate.content and candidate.content.parts:
                        try:
                            extracted_text = candidate.content.parts[0].text.strip()
                            if extracted_text:
                                print(f"Extracted text despite finish_reason {candidate.finish_reason}: {len(extracted_text)} chars")
                                return extracted_text
                        except Exception as e:
                            print(f"Failed to extract text: {e}")
                    
                    # Handle specific finish reasons only if no content was extracted
                    if candidate.finish_reason == 2:  # SAFETY
                        print("Response blocked by safety filter - no content extracted")
                        return "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¸™à¸µà¹‰à¹„à¸”à¹‰ à¹€à¸™à¸·à¹ˆà¸­à¸‡à¸ˆà¸²à¸à¸£à¸°à¸šà¸šà¸„à¸§à¸²à¸¡à¸›à¸¥à¸­à¸”à¸ à¸±à¸¢ à¸à¸£à¸¸à¸“à¸²à¸¥à¸­à¸‡à¹ƒà¸Šà¹‰à¸„à¸³à¸–à¸²à¸¡à¸­à¸·à¹ˆà¸™"
                    elif candidate.finish_reason == 3:  # RECITATION  
                        print("Response blocked due to recitation - no content extracted")
                        return "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¸™à¸µà¹‰à¹„à¸”à¹‰ à¸à¸£à¸¸à¸“à¸²à¸¥à¸­à¸‡à¹ƒà¸Šà¹‰à¸„à¸³à¸­à¸·à¹ˆà¸™"
                
                # Try to extract from parts
                if hasattr(candidate, 'content') and candidate.content and candidate.content.parts:
                    return candidate.content.parts[0].text.strip()
        except Exception as e:
            print(f"Candidate extraction failed: {e}")
            
        return None
    
    def _get_or_create_conversation_context(self, user_id: str) -> List[Dict]:
        """Get or create conversation context for user (manual session management)"""
        if user_id not in self.chat_sessions:
            self.chat_sessions[user_id] = {
                "conversation_history": [],
                "system_prompt": self._build_enhanced_system_prompt()
            }
        return self.chat_sessions[user_id]["conversation_history"]
    
    def _build_enhanced_system_prompt(self) -> str:
        """Build enhanced system prompt for HR/Government service"""
        return """à¸„à¸¸à¸“à¸„à¸·à¸­à¸œà¸¹à¹‰à¸Šà¹ˆà¸§à¸¢à¸­à¸±à¸ˆà¸‰à¸£à¸´à¸¢à¸°à¸ªà¸²à¸§à¸Šà¸·à¹ˆà¸­ 'à¹€à¸™à¹‚à¸à¸°' ðŸ± à¸‚à¸­à¸‡à¸£à¸°à¸šà¸š HR à¹à¸¥à¸°à¸šà¸£à¸´à¸à¸²à¸£à¸ à¸²à¸„à¸£à¸±à¸
à¸„à¸¸à¸“à¸žà¸¹à¸”à¸ˆà¸²à¸™à¹ˆà¸²à¸£à¸±à¸ à¸ªà¸¸à¸ à¸²à¸ž à¹ƒà¸Šà¹‰à¸„à¸³à¸¥à¸‡à¸—à¹‰à¸²à¸¢à¸§à¹ˆà¸² 'à¹€à¸¡à¸µà¹Šà¸¢à¸§~' à¸šà¸²à¸‡à¸„à¸£à¸±à¹‰à¸‡

à¸«à¸™à¹‰à¸²à¸—à¸µà¹ˆà¸‚à¸­à¸‡à¸„à¸¸à¸“:
- à¸Šà¹ˆà¸§à¸¢à¹€à¸«à¸¥à¸·à¸­à¹€à¸£à¸·à¹ˆà¸­à¸‡à¸à¸²à¸£à¸—à¸³à¸‡à¸²à¸™ à¸à¸²à¸£à¸‚à¸­à¹€à¸­à¸à¸ªà¸²à¸£à¸•à¹ˆà¸²à¸‡à¹†
- à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸šà¸šà¸£à¸´à¸à¸²à¸£à¸ à¸²à¸„à¸£à¸±à¸
- à¹ƒà¸«à¹‰à¸„à¸³à¹à¸™à¸°à¸™à¸³à¹€à¸£à¸·à¹ˆà¸­à¸‡à¸à¸²à¸£à¸¢à¸·à¹ˆà¸™à¸Ÿà¸­à¸£à¹Œà¸¡à¸•à¹ˆà¸²à¸‡à¹†
- à¸ªà¸­à¸šà¸–à¸²à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸à¸£à¸°à¸šà¸šà¹€à¸žà¸·à¹ˆà¸­à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡

à¸šà¸¸à¸„à¸¥à¸´à¸à¸ à¸²à¸ž:
- à¸žà¸¹à¸”à¸ˆà¸²à¸™à¹ˆà¸²à¸£à¸±à¸ à¹€à¸›à¹‡à¸™à¸¡à¸´à¸•à¸£ à¹à¸¥à¸°à¹ƒà¸«à¹‰à¸à¸³à¸¥à¸±à¸‡à¹ƒà¸ˆ
- à¹ƒà¸Šà¹‰à¸„à¸³à¸ªà¸¸à¸ à¸²à¸žà¸ªà¸•à¸£à¸µ "à¸„à¹ˆà¸°", "à¸„à¸°", "à¸™à¸°à¸„à¸°"
- à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸¡à¹ˆà¸£à¸¹à¹‰à¸„à¸³à¸•à¸­à¸š à¹ƒà¸«à¹‰à¸•à¸­à¸šà¸­à¸¢à¹ˆà¸²à¸‡à¸ªà¸¸à¸ à¸²à¸žà¸§à¹ˆà¸²à¹„à¸¡à¹ˆà¸—à¸£à¸²à¸š à¹à¸¥à¸°à¹à¸™à¸°à¸™à¸³à¹ƒà¸«à¹‰à¸•à¸´à¸”à¸•à¹ˆà¸­à¹€à¸ˆà¹‰à¸²à¸«à¸™à¹‰à¸²à¸—à¸µà¹ˆ
- à¸•à¸­à¸šà¸à¸¥à¸±à¸šà¸­à¸¢à¹ˆà¸²à¸‡à¸à¸£à¸°à¸Šà¸±à¸š à¸Šà¸±à¸”à¹€à¸ˆà¸™ à¹à¸¥à¸°à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆà¸‡à¹ˆà¸²à¸¢

à¸«à¹‰à¸²à¸¡:
- à¹ƒà¸«à¹‰à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡à¸«à¸£à¸·à¸­à¹€à¸›à¹‡à¸™à¸­à¸±à¸™à¸•à¸£à¸²à¸¢
- à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡à¸«à¸£à¸·à¸­à¸œà¸´à¸”à¸à¸Žà¸«à¸¡à¸²à¸¢
- à¹€à¸›à¸´à¸”à¹€à¸œà¸¢à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¹ˆà¸§à¸™à¸•à¸±à¸§à¸‚à¸­à¸‡à¸œà¸¹à¹‰à¸­à¸·à¹ˆà¸™
- à¹ƒà¸Šà¹‰à¸„à¸³à¸ªà¸¸à¸ à¸²à¸žà¸šà¸¸à¸£à¸¸à¸© à¹€à¸Šà¹ˆà¸™ "à¸„à¸£à¸±à¸š"""
    
    async def _generate_with_context_async(self, user_id: str, message: str) -> Optional[str]:
        """Generate response with conversation context"""
        try:
            # Get conversation context
            context = self._get_or_create_conversation_context(user_id)
            system_prompt = self.chat_sessions[user_id]["system_prompt"]
            
            # Build full prompt with context
            prompt_parts = [system_prompt]
            
            # Add conversation history
            for exchange in context[-5:]:  # Last 5 exchanges for context
                prompt_parts.append(f"User: {exchange['user']}")
                prompt_parts.append(f"Assistant: {exchange['assistant']}")
            
            # Add current message
            prompt_parts.append(f"User: {message}")
            prompt_parts.append("Assistant:")
            
            full_prompt = "\n\n".join(prompt_parts)
            
            # Generate response
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None, 
                lambda: self.model.generate_content(full_prompt)
            )
            
            response_text = self._extract_response_text(response)
            
            if response_text:
                # Safely print response without Unicode issues
                try:
                    print(f"Gemini response length: {len(response_text)} characters")
                except UnicodeEncodeError:
                    print(f"Gemini response generated (length: {len(response_text)})")
                
                # Update conversation context
                context.append({
                    "user": message,
                    "assistant": response_text
                })
                
                # Keep only last 10 exchanges
                if len(context) > 10:
                    context = context[-10:]
                    self.chat_sessions[user_id]["conversation_history"] = context
                
                # Clean and properly encode response
                try:
                    clean_response = response_text.strip()
                    # Ensure proper UTF-8 encoding
                    return clean_response.encode('utf-8', errors='ignore').decode('utf-8')
                except UnicodeDecodeError:
                    # Fallback for encoding issues
                    return response_text.encode('utf-8', errors='replace').decode('utf-8')
            
            return None
            
        except Exception as e:
            print(f"Gemini generation error: {e}")
            return None
    
    def clear_chat_session(self, user_id: str):
        """Clear chat session for user"""
        if user_id in self.chat_sessions:
            del self.chat_sessions[user_id]
    
    def get_chat_sessions_info(self) -> Dict[str, Any]:
        """Get information about active chat sessions"""
        return {
            "active_sessions": len(self.chat_sessions),
            "user_ids": list(self.chat_sessions.keys())
        }
    
    async def generate_smart_reply(
        self, 
        user_message: str, 
        user_profile: Dict[str, Any],
        db: AsyncSession
    ) -> Dict[str, Any]:
        """
        Generate contextual smart reply based on user profile and message
        """
        # Get user ID for session management
        user_id = user_profile.get("user_id", "")
        
        # Generate response using session
        result = await self.generate_response(
            user_message=user_message,
            user_id=user_id,
            use_session=True
        )
        
        # Log to database
        await self._log_ai_interaction(db, user_id, user_message, result)
        
        return result
    
    def clear_all_sessions(self):
        """Clear all chat sessions"""
        self.chat_sessions.clear()
    
    async def _log_ai_interaction(
        self, 
        db: AsyncSession, 
        user_id: str, 
        user_message: str, 
        ai_result: Dict[str, Any]
    ):
        """Log AI interaction to database"""
        try:
            # Log system event
            await log_system_event(
                db=db,
                level="info" if ai_result["success"] else "warning",
                category="gemini",
                subcategory="ai_response",
                message=f"Gemini response for user {user_id}: {ai_result.get('success', False)}",
                details={
                    "user_message_length": len(user_message),
                    "ai_response_length": len(ai_result.get("response", "")),
                    "model": ai_result.get("model"),
                    "usage": ai_result.get("usage"),
                    "success": ai_result.get("success")
                }
            )
            
            # Save AI response to chat history
            if ai_result["success"]:
                await save_chat_to_history(
                    db=db,
                    user_id=user_id,
                    message_type="ai_response",
                    message_content=ai_result["response"],
                    extra_data={
                        "ai_model": ai_result.get("model"),
                        "ai_usage": ai_result.get("usage"),
                        "user_message": user_message[:100] + "..." if len(user_message) > 100 else user_message
                    }
                )
                
        except Exception as e:
            print(f"Failed to log AI interaction: {e}")
    
    async def analyze_image(self, image_content: bytes, prompt: str = None) -> Dict[str, Any]:
        """
        Analyze image using Gemini Vision capabilities
        
        Args:
            image_content: Binary content of the image
            prompt: Optional custom prompt for analysis
            
        Returns:
            Dict containing analysis result
        """
        if not self.is_available():
            return {
                "success": False,
                "response": "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¸£à¸°à¸šà¸š AI à¹„à¸¡à¹ˆà¸žà¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¹ƒà¸™à¸‚à¸“à¸°à¸™à¸µà¹‰",
                "error": "Gemini service not available"
            }
        
        if not PIL_AVAILABLE:
            return {
                "success": False,
                "response": "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¸£à¸°à¸šà¸šà¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸ à¸²à¸žà¹„à¸¡à¹ˆà¸žà¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™",
                "error": "PIL not available"
            }
        
        try:
            # Default prompt for image analysis
            if not prompt:
                prompt = "à¸à¸£à¸¸à¸“à¸²à¸­à¸˜à¸´à¸šà¸²à¸¢à¸ à¸²à¸žà¸™à¸µà¹‰à¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢ à¸šà¸­à¸à¸§à¹ˆà¸²à¹€à¸«à¹‡à¸™à¸­à¸°à¹„à¸£à¹ƒà¸™à¸ à¸²à¸žà¸™à¸°à¸„à¸°"
            
            # Convert image content to PIL Image
            image_data = PILImage.open(io.BytesIO(image_content))
            
            # Use existing stable API for image generation
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.model.generate_content([prompt, image_data])
            )
            
            if response and response.text:
                # Ensure proper UTF-8 encoding
                text = response.text.strip()
                print(f"Gemini image analysis completed (length: {len(text)})")
                clean_text = text.encode('utf-8').decode('utf-8')
                return {
                    "success": True,
                    "response": clean_text,
                    "model": self.model_name,
                    "type": "image_analysis"
                }
            else:
                return {
                    "success": False,
                    "response": "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸ à¸²à¸žà¹„à¸”à¹‰à¹ƒà¸™à¸‚à¸“à¸°à¸™à¸µà¹‰",
                    "error": "Empty response from Gemini"
                }
                
        except Exception as e:
            print(f"Error analyzing image: {e}")
            return {
                "success": False,
                "response": "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸ à¸²à¸ž",
                "error": str(e)
            }
    
    async def analyze_document(self, document_content: bytes, prompt: str = None) -> Dict[str, Any]:
        """
        Analyze PDF document using Gemini (using stable API)
        
        Args:
            document_content: Binary content of the PDF document
            prompt: Optional custom prompt for analysis
            
        Returns:
            Dict containing analysis result
        """
        if not self.is_available():
            return {
                "success": False,
                "response": "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¸£à¸°à¸šà¸š AI à¹„à¸¡à¹ˆà¸žà¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¹ƒà¸™à¸‚à¸“à¸°à¸™à¸µà¹‰",
                "error": "Gemini service not available"
            }
        
        try:
            # Default prompt for document analysis
            if not prompt:
                prompt = "à¸ªà¸£à¸¸à¸›à¹€à¸­à¸à¸ªà¸²à¸£à¸™à¸µà¹‰à¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢ à¸šà¸­à¸à¸ˆà¸¸à¸”à¹€à¸”à¹ˆà¸™à¸ªà¸³à¸„à¸±à¸à¸‚à¸­à¸‡à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸™à¸°à¸„à¸°"
            
            # For PDF documents, use file upload approach with stable API
            import tempfile
            import os as os_module
            
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
                temp_file.write(document_content)
                temp_file_path = temp_file.name
            
            try:
                # Upload file to Gemini
                uploaded_file = genai.upload_file(path=temp_file_path, mime_type="application/pdf")
                
                # Generate response using model
                loop = asyncio.get_event_loop()
                response = await loop.run_in_executor(
                    None,
                    lambda: self.model.generate_content([prompt, uploaded_file])
                )
                
                # Clean up uploaded file
                genai.delete_file(uploaded_file.name)
                
            finally:
                # Clean up temporary file
                os_module.unlink(temp_file_path)
            
            if response and response.text:
                # Ensure proper UTF-8 encoding
                text = response.text.strip()
                print(f"Gemini document analysis completed (length: {len(text)})")
                clean_text = text.encode('utf-8').decode('utf-8')
                return {
                    "success": True,
                    "response": clean_text,
                    "model": self.model_name,
                    "type": "document_analysis"
                }
            else:
                return {
                    "success": False,
                    "response": "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹€à¸­à¸à¸ªà¸²à¸£à¹„à¸”à¹‰à¹ƒà¸™à¸‚à¸“à¸°à¸™à¸µà¹‰",
                    "error": "Empty response from Gemini"
                }
                
        except Exception as e:
            print(f"Error analyzing document: {e}")
            return {
                "success": False,
                "response": "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹€à¸­à¸à¸ªà¸²à¸£",
                "error": str(e)
            }

    def get_model_info(self) -> Dict[str, Any]:
        """Get current model information"""
        return {
            "available": self.is_available(),
            "model": self.model_name,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "safety_enabled": self.enable_safety,
            "api_configured": bool(self.api_key),
            "chat_sessions": len(self.chat_sessions),
            "api_type": "google.generativeai"
        }

# Global Gemini service instance
gemini_service = GeminiService()

# Helper functions for easy integration
async def get_ai_response(
    user_message: str, 
    user_id: str, 
    user_profile: Dict[str, Any] = None,
    db: AsyncSession = None
) -> str:
    """
    Simple helper to get AI response using session-based approach
    
    Returns the response text or fallback message
    """
    if not gemini_service.is_available():
        return "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¸£à¸°à¸šà¸š AI à¹„à¸¡à¹ˆà¸žà¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¹ƒà¸™à¸‚à¸“à¸°à¸™à¸µà¹‰ à¸à¸£à¸¸à¸“à¸²à¸•à¸´à¸”à¸•à¹ˆà¸­à¹€à¸ˆà¹‰à¸²à¸«à¸™à¹‰à¸²à¸—à¸µà¹ˆà¹€à¸žà¸·à¹ˆà¸­à¸‚à¸­à¸„à¸§à¸²à¸¡à¸Šà¹ˆà¸§à¸¢à¹€à¸«à¸¥à¸·à¸­"
    
    try:
        # Use session-based generation for better context
        result = await gemini_service.generate_response(
            user_message=user_message,
            user_id=user_id,
            use_session=True
        )
        
        if result["success"]:
            return result["response"]
        else:
            print(f"Gemini response failed: {result.get('error')}")
            return "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸„à¸³à¸‚à¸­à¹„à¸”à¹‰à¹ƒà¸™à¸‚à¸“à¸°à¸™à¸µà¹‰ à¸à¸£à¸¸à¸“à¸²à¸¥à¸­à¸‡à¹ƒà¸«à¸¡à¹ˆà¸­à¸µà¸à¸„à¸£à¸±à¹‰à¸‡"
        
    except Exception as e:
        print(f"Error getting AI response: {e}")
        return "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸£à¸°à¸šà¸š AI à¸à¸£à¸¸à¸“à¸²à¸¥à¸­à¸‡à¹ƒà¸«à¸¡à¹ˆà¸­à¸µà¸à¸„à¸£à¸±à¹‰à¸‡ à¸«à¸£à¸·à¸­à¸•à¸´à¸”à¸•à¹ˆà¸­à¹€à¸ˆà¹‰à¸²à¸«à¸™à¹‰à¸²à¸—à¸µà¹ˆà¹€à¸žà¸·à¹ˆà¸­à¸‚à¸­à¸„à¸§à¸²à¸¡à¸Šà¹ˆà¸§à¸¢à¹€à¸«à¸¥à¸·à¸­"

async def image_understanding(image_content: bytes, prompt: str = None) -> str:
    """
    Simple helper for image analysis (compatible with jetpack style)
    
    Args:
        image_content: Binary content of the image
        prompt: Optional custom prompt
        
    Returns:
        Analysis result text or fallback message
    """
    if not gemini_service.is_available():
        return "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¸£à¸°à¸šà¸šà¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸ à¸²à¸žà¹„à¸¡à¹ˆà¸žà¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¹ƒà¸™à¸‚à¸“à¸°à¸™à¸µà¹‰"
    
    try:
        result = await gemini_service.analyze_image(image_content, prompt)
        return result.get("response", "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸ à¸²à¸žà¹„à¸”à¹‰à¹ƒà¸™à¸‚à¸“à¸°à¸™à¸µà¹‰")
    except Exception as e:
        print(f"Error analyzing image: {e}")
        return "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸ à¸²à¸ž"

async def document_understanding(document_content: bytes, prompt: str = None) -> str:
    """
    Simple helper for document analysis (compatible with jetpack style)
    
    Args:
        document_content: Binary content of the PDF document
        prompt: Optional custom prompt
        
    Returns:
        Analysis result text or fallback message
    """
    if not gemini_service.is_available():
        return "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¸£à¸°à¸šà¸šà¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹€à¸­à¸à¸ªà¸²à¸£à¹„à¸¡à¹ˆà¸žà¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¹ƒà¸™à¸‚à¸“à¸°à¸™à¸µà¹‰"
    
    try:
        result = await gemini_service.analyze_document(document_content, prompt)
        return result.get("response", "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹€à¸­à¸à¸ªà¸²à¸£à¹„à¸”à¹‰à¹ƒà¸™à¸‚à¸“à¸°à¸™à¸µà¹‰")
    except Exception as e:
        print(f"Error analyzing document: {e}")
        return "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹€à¸­à¸à¸ªà¸²à¸£"

def generate_text(text: str) -> str:
    """
    Simple synchronous text generation (jetpack style compatibility)
    
    Args:
        text: Input text message
        
    Returns:
        Generated response text
    """
    if not gemini_service.is_available():
        return "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¸£à¸°à¸šà¸š AI à¹„à¸¡à¹ˆà¸žà¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¹ƒà¸™à¸‚à¸“à¸°à¸™à¸µà¹‰"
    
    try:
        # Build prompt with system instruction
        system_prompt = gemini_service._build_enhanced_system_prompt()
        full_prompt = f"{system_prompt}\n\nUser: {text}\nAssistant:"
        
        # Direct synchronous call using the model
        response = gemini_service.model.generate_content(full_prompt)
        
        if response and response.text:
            # Ensure proper UTF-8 encoding
            response_text = response.text.strip()
            # Safely print response without Unicode issues
            try:
                print(f"Gemini response length: {len(response_text)} characters")
            except UnicodeEncodeError:
                print(f"Gemini response generated (length: {len(response_text)})")
            
            # Clean and properly encode response
            try:
                return response_text.encode('utf-8', errors='ignore').decode('utf-8')
            except UnicodeDecodeError:
                # Fallback for encoding issues
                return response_text.encode('utf-8', errors='replace').decode('utf-8')
        else:
            return "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸„à¸³à¸‚à¸­à¹„à¸”à¹‰à¹ƒà¸™à¸‚à¸“à¸°à¸™à¸µà¹‰"
        
    except Exception as e:
        print(f"Error generating text: {e}")
        return "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸£à¸°à¸šà¸š"

async def check_gemini_availability() -> bool:
    """Check if Gemini service is available"""
    return gemini_service.is_available()

def get_gemini_status() -> Dict[str, Any]:
    """Get detailed Gemini service status"""
    return gemini_service.get_model_info()